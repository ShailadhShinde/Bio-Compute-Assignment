{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmpzyaidgreH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class DeepSignal(nn.Module):\n",
        "    def __init__(self, sequence_feature_size=128, signal_feature_size=512, num_classes=2):\n",
        "        super(DeepSignal, self).__init__()\n",
        "\n",
        "        # Sequence Feature Module (BRNN)\n",
        "        self.sequence_brnn = nn.LSTM(\n",
        "            input_size=4,  # nucleotide type, mean, std, num_signals\n",
        "            hidden_size=sequence_feature_size // 2,  # //2 for bidirectional\n",
        "            num_layers=3,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Signal Feature Module (CNN with Inception blocks)\n",
        "        self.signal_cnn = SignalCNN()\n",
        "\n",
        "        # Classification Module\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(sequence_feature_size + signal_feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, sequence_features, signal_features):\n",
        "        # Sequence features: (batch_size, seq_len=17, features=4)\n",
        "        sequence_out, (h_n, c_n) = self.sequence_brnn(sequence_features)\n",
        "        # Use the last hidden state from both directions\n",
        "        sequence_features = torch.cat([h_n[-2], h_n[-1]], dim=1)  # (batch_size, sequence_feature_size)\n",
        "\n",
        "        # Signal features: (batch_size, 1, 360)\n",
        "        signal_features = self.signal_cnn(signal_features)\n",
        "\n",
        "        # Concatenate features\n",
        "        combined_features = torch.cat([sequence_features, signal_features], dim=1)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(combined_features)\n",
        "\n",
        "        return output\n",
        "\n",
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "\n",
        "        # 1x1 convolution branch\n",
        "        self.branch1 = nn.Conv1d(in_channels, out_channels//5, kernel_size=1)\n",
        "\n",
        "        # 1x3 convolution branch\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels//5, kernel_size=1),\n",
        "            nn.Conv1d(out_channels//5, out_channels//5, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        # 1x5 convolution branch\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels//5, kernel_size=1),\n",
        "            nn.Conv1d(out_channels//5, out_channels//5, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "        # Residual 1x3 convolution branch\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels//5, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "        # 1x3 maxpool branch\n",
        "        self.branch5 = nn.Sequential(\n",
        "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv1d(in_channels, out_channels//5, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1 = self.branch1(x)\n",
        "        branch2 = self.branch2(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch4 = self.branch4(x) + x  # residual connection\n",
        "        branch5 = self.branch5(x)\n",
        "\n",
        "        out = torch.cat([branch1, branch2, branch3, branch4, branch5], dim=1)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class SignalCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SignalCNN, self).__init__()\n",
        "\n",
        "        # Initial layers\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        # Inception blocks (11 total as mentioned in paper)\n",
        "        self.inception_blocks = nn.Sequential(\n",
        "            InceptionBlock(64, 64),\n",
        "            InceptionBlock(64, 64),\n",
        "            InceptionBlock(64, 64),  # 3x inception\n",
        "\n",
        "            InceptionBlock(64, 128),\n",
        "            InceptionBlock(128, 128),\n",
        "            InceptionBlock(128, 128),\n",
        "            InceptionBlock(128, 128),\n",
        "            InceptionBlock(128, 128),  # 5x inception\n",
        "\n",
        "            InceptionBlock(128, 256),\n",
        "            InceptionBlock(256, 256),\n",
        "            InceptionBlock(256, 256),\n",
        "            InceptionBlock(256, 256),\n",
        "            InceptionBlock(256, 512),  # 5x inception\n",
        "        )\n",
        "\n",
        "        # Final layers\n",
        "        self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, 1, 360)\n",
        "        x = self.initial_conv(x)\n",
        "        x = self.inception_blocks(x)\n",
        "        x = self.final_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MethylationDataset(Dataset):\n",
        "    def __init__(self, sequence_features, signal_features, labels):\n",
        "        \"\"\"\n",
        "        sequence_features: numpy array of shape (num_samples, 17, 4)\n",
        "        signal_features: numpy array of shape (num_samples, 360)\n",
        "        labels: numpy array of shape (num_samples,) with 0/1 labels\n",
        "        \"\"\"\n",
        "        self.sequence_features = torch.FloatTensor(sequence_features)\n",
        "        self.signal_features = torch.FloatTensor(signal_features).unsqueeze(1)  # Add channel dimension\n",
        "        self.labels = torch.FloatTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequence_features[idx], self.signal_features[idx], self.labels[idx]\n",
        "\n",
        "class DeepSignalTrainer:\n",
        "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "        # Loss function and optimizer as described in paper\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "    def train_epoch(self, dataloader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (seq_features, sig_features, labels) in enumerate(dataloader):\n",
        "            seq_features = seq_features.to(self.device)\n",
        "            sig_features = sig_features.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(seq_features, sig_features)\n",
        "\n",
        "            # Convert to probabilities using sigmoid and normalize\n",
        "            prob_methylated = torch.sigmoid(outputs[:, 0])\n",
        "            prob_unmethylated = torch.sigmoid(outputs[:, 1])\n",
        "\n",
        "            total_prob = prob_methylated + prob_unmethylated\n",
        "            final_output_methylated = prob_methylated / total_prob\n",
        "\n",
        "            loss = self.criterion(final_output_methylated, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=50, early_stopping_patience=5):\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss = self.train_epoch(train_loader)\n",
        "            val_loss = self.validate(val_loader)\n",
        "\n",
        "            self.scheduler.step()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}:')\n",
        "            print(f'  Train Loss: {train_loss:.4f}')\n",
        "            print(f'  Val Loss: {val_loss:.4f}')\n",
        "            print(f'  LR: {self.optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(self.model.state_dict(), 'deepsignal_best_model.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= early_stopping_patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    def validate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for seq_features, sig_features, labels in dataloader:\n",
        "                seq_features = seq_features.to(self.device)\n",
        "                sig_features = sig_features.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(seq_features, sig_features)\n",
        "\n",
        "                prob_methylated = torch.sigmoid(outputs[:, 0])\n",
        "                prob_unmethylated = torch.sigmoid(outputs[:, 1])\n",
        "\n",
        "                total_prob = prob_methylated + prob_unmethylated\n",
        "                final_output_methylated = prob_methylated / total_prob\n",
        "\n",
        "                loss = self.criterion(final_output_methylated, labels)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "def main():\n",
        "\n",
        "    train_dataset = MethylationDataset(sequence_train, signal_train, labels_train)\n",
        "    val_dataset = MethylationDataset(sequence_val, signal_val, labels_val)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "    model = DeepSignal()\n",
        "\n",
        "    trainer = DeepSignalTrainer(model)\n",
        "\n",
        "    trainer.train(train_loader, val_loader, epochs=50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NO Testing cause limited data, although  can use similar signals to test after passing through the pipeline. Like download another raw signals and test on the pipeline"
      ],
      "metadata": {
        "id": "MB2d-ElHh31v"
      }
    }
  ]
}