{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLHV-gJTnMfJ",
        "outputId": "f007d796-3a0a-4351-d30e-c1cc9ca7b2f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pod5\n",
            "  Downloading pod5-0.3.35-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pysam\n",
            "  Downloading pysam-0.23.3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting deprecated~=1.2.18 (from pod5)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting lib_pod5==0.3.35 (from pod5)\n",
            "  Downloading lib_pod5-0.3.35-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting iso8601 (from pod5)\n",
            "  Downloading iso8601-2.1.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.12/dist-packages (from pod5) (10.8.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from pod5) (2.0.2)\n",
            "Collecting pyarrow~=22.0.0 (from pod5)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from pod5) (2025.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pod5) (25.0)\n",
            "Requirement already satisfied: polars~=1.30 in /usr/local/lib/python3.12/dist-packages (from pod5) (1.31.0)\n",
            "Requirement already satisfied: h5py~=3.11 in /usr/local/lib/python3.12/dist-packages (from pod5) (3.15.1)\n",
            "Collecting vbz_h5py_plugin (from pod5)\n",
            "  Downloading vbz_h5py_plugin-1.0.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from pod5) (4.67.1)\n",
            "Collecting wrapt<2,>=1.10 (from deprecated~=1.2.18->pod5)\n",
            "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading pod5-0.3.35-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lib_pod5-0.3.35-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysam-0.23.3-cp312-cp312-manylinux_2_28_x86_64.whl (24.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iso8601-2.1.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading vbz_h5py_plugin-1.0.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, pysam, pyarrow, lib_pod5, iso8601, vbz_h5py_plugin, deprecated, pod5\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 2.0.1\n",
            "    Uninstalling wrapt-2.0.1:\n",
            "      Successfully uninstalled wrapt-2.0.1\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "Successfully installed deprecated-1.2.18 iso8601-2.1.0 lib_pod5-0.3.35 pod5-0.3.35 pyarrow-22.0.0 pysam-0.23.3 vbz_h5py_plugin-1.0.1 wrapt-1.17.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pod5 pysam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ygkb-hG4mChK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9607a7af-5ad8-4231-a5ca-8d7feebfd5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Downloading reference genome...\n",
            "Ref ready: /content/deepsignal_paper/listeria_ref.fa\n",
            "Installing Dorado...\n",
            "Moved /content/dorado-1.2.0-linux-x64 -> /content/dorado\n",
            "Dorado version: [2025-11-21 23:48:36.950] [info] Running: \"--version\"\n",
            "1.2.0+f9443bb8\n",
            "Downloading model...\n",
            "Downloading base dna_r10.4.1_e8.2_400bps_hac@v5.0.0...\n",
            "Base model ready: /content/.dorado/models/dna_r10.4.1_e8.2_400bps_hac@v5.0.0\n",
            "Processing POD5 files...\n",
            "Found 3 POD5s\n",
            "Processing ATCC_19119__202309_batch1.pod5\n",
            "Running Dorado...\n",
            "Raw BAM created: /content/deepsignal_paper/batch_0.bam\n",
            "Sorting BAM...\n",
            "Indexing BAM...\n",
            "Final sorted BAM: /content/deepsignal_paper/batch_0.sorted.bam (1039 alignments)\n",
            "Processing ATCC_19119__202309_batch86.pod5\n",
            "Running Dorado...\n",
            "Raw BAM created: /content/deepsignal_paper/batch_1.bam\n",
            "Sorting BAM...\n",
            "Indexing BAM...\n",
            "Final sorted BAM: /content/deepsignal_paper/batch_1.sorted.bam (1143 alignments)\n",
            "Processing ATCC_19119__202309_batch90.pod5\n",
            "Running Dorado...\n",
            "Raw BAM created: /content/deepsignal_paper/batch_2.bam\n",
            "Sorting BAM...\n",
            "Indexing BAM...\n",
            "Final sorted BAM: /content/deepsignal_paper/batch_2.sorted.bam (1036 alignments)\n",
            "Merged BAM created: /content/deepsignal_paper/all_pod5s_merged.bam\n",
            "Processing complete!\n"
          ]
        }
      ],
      "source": [
        "# Imports & Global Setup\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "import pod5\n",
        "import torch\n",
        "INPUT_DIR = \"/content/drive/MyDrive/Untitled folder/pod5\"   # Here is your pod5 path\n",
        "POD5_GLOB = \"/content/drive/MyDrive/Untitled folder/pod5/*.pod5\"   # Here is your pod5 path\n",
        "\n",
        "OUTPUT_DIR = \"/content/deepsignal_paper\"\n",
        "MODEL_NAME = \"dna_r10.4.1_e8.2_400bps_hac@v5.0.0\"\n",
        "MODS = \"6mA\"\n",
        "MODEL_CACHE = \"/content/.dorado/models\"\n",
        "REF_PATH = os.path.join(OUTPUT_DIR, \"listeria_ref.fa\")\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "MIN_QSCORE = 0\n",
        "BATCHSIZE = 32 if \"cuda\" in DEVICE else 2\n",
        "MAX_READS = 1000\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_CACHE, exist_ok=True)\n",
        "\n",
        "def run_cmd(cmd, capture_output=True, shell=False):\n",
        "    env = os.environ.copy()\n",
        "    env['PATH'] = f\"/content/dorado/bin:{env.get('PATH', '')}\"\n",
        "    if isinstance(cmd, str):\n",
        "        if shell:\n",
        "            proc = subprocess.run(cmd, shell=True, capture_output=capture_output, text=True, env=env)\n",
        "        else:\n",
        "            cmd = cmd.split()\n",
        "            proc = subprocess.run(cmd, capture_output=capture_output, text=True, env=env)\n",
        "    else:\n",
        "        proc = subprocess.run(cmd, capture_output=capture_output, text=True, env=env)\n",
        "    if proc.returncode != 0:\n",
        "        stderr_snip = proc.stderr[:300] if proc.stderr is not None else ''\n",
        "        print(f\"RC={proc.returncode}: {stderr_snip}\")\n",
        "    return proc.returncode, proc.stdout, proc.stderr\n",
        "\n",
        "# Install Dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "run_cmd([\"apt\", \"update\", \"-qq\"])\n",
        "run_cmd([\"apt\", \"install\", \"-y\", \"samtools\"])\n",
        "run_cmd([\"pip\", \"install\", \"pod5\", \"pysam\"])\n",
        "\n",
        "# GPU check\n",
        "try:\n",
        "    run_cmd([\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv\"], capture_output=False)\n",
        "except FileNotFoundError:\n",
        "    print(\"No GPU - using CPU\")\n",
        "\n",
        "# Download Reference Genome\n",
        "print(\"Downloading reference genome...\")\n",
        "ref_url = \"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/196/035/GCF_000196035.1_ASM19603v1/GCF_000196035.1_ASM19603v1_genomic.fna.gz\"\n",
        "if not os.path.exists(REF_PATH):\n",
        "    run_cmd([\"wget\", \"-q\", \"-O\", f\"{REF_PATH}.gz\", ref_url])\n",
        "    run_cmd([\"gunzip\", \"-f\", f\"{REF_PATH}.gz\"])\n",
        "    run_cmd([\"samtools\", \"faidx\", REF_PATH])\n",
        "print(f\"Ref ready: {REF_PATH}\")\n",
        "\n",
        "# Dorado Install\n",
        "print(\"Installing Dorado...\")\n",
        "dorado_bin = \"/content/dorado/bin/dorado\"\n",
        "if not os.path.exists(dorado_bin):\n",
        "    run_cmd(\n",
        "        \"wget -qO dorado.tar.gz \"\n",
        "        \"https://cdn.oxfordnanoportal.com/software/analysis/dorado-1.2.0-linux-x64.tar.gz\",\n",
        "        shell=True\n",
        "    )\n",
        "    run_cmd(\"tar -xzf dorado.tar.gz\", shell=True)\n",
        "    folders = glob.glob(\"/content/dorado-1.2.0*\")\n",
        "    if len(folders) == 0:\n",
        "        raise RuntimeError(\"Dorado extraction failed\")\n",
        "    extracted = folders[0]\n",
        "    os.rename(extracted, \"/content/dorado\")\n",
        "    print(f\"Moved {extracted} -> /content/dorado\")\n",
        "rc, out, err = run_cmd(\"dorado --version\")\n",
        "print(\"Dorado version:\", (out or err).strip())\n",
        "\n",
        "# Model Download\n",
        "print(\"Downloading model...\")\n",
        "base_model = \"dna_r10.4.1_e8.2_400bps_hac@v5.0.0\"\n",
        "base_path = os.path.join(MODEL_CACHE, base_model)\n",
        "if not os.path.exists(base_path):\n",
        "    print(f\"Downloading base {base_model}...\")\n",
        "    rc, _, _ = run_cmd(f\"dorado download --model {base_model} --models-directory {MODEL_CACHE}\")\n",
        "    if rc != 0:\n",
        "        tar_file = f\"{base_model}.tar.gz\"\n",
        "        cdn_url = f\"https://resources.nanoporetech.com/models/{base_model}.tar.gz\"\n",
        "        run_cmd(f\"wget -qO {tar_file} {cdn_url}\", shell=True)\n",
        "        run_cmd(f\"tar -xzf {tar_file} -C {MODEL_CACHE}\", shell=True)\n",
        "        os.remove(tar_file)\n",
        "print(f\"Base model ready: {base_path}\")\n",
        "\n",
        "# Process POD5 Files\n",
        "print(\"Processing POD5 files...\")\n",
        "pod5_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"*.pod5\")))\n",
        "if not pod5_files:\n",
        "    raise FileNotFoundError(\"No POD5s found\")\n",
        "print(f\"Found {len(pod5_files)} POD5s\")\n",
        "\n",
        "bams = []\n",
        "for i, pod5_file in enumerate(pod5_files):\n",
        "    base = os.path.basename(pod5_file)\n",
        "    print(f\"Processing {base}\")\n",
        "    bam_raw = os.path.join(OUTPUT_DIR, f\"batch_{i}.bam\")\n",
        "    bam_sorted = os.path.join(OUTPUT_DIR, f\"batch_{i}.sorted.bam\")\n",
        "\n",
        "    if os.path.exists(bam_sorted) and os.path.getsize(bam_sorted) > 5000:\n",
        "        print(\"Skipping - sorted BAM exists\")\n",
        "        bams.append(bam_sorted)\n",
        "        continue\n",
        "\n",
        "    cmd = [\n",
        "        \"/content/dorado/bin/dorado\", \"basecaller\",\n",
        "        MODEL_NAME,\n",
        "        pod5_file,\n",
        "        \"--reference\", REF_PATH,\n",
        "        \"--modified-bases\", MODS,\n",
        "        \"--device\", DEVICE,\n",
        "        \"--min-qscore\", str(MIN_QSCORE),\n",
        "        \"--batchsize\", str(BATCHSIZE),\n",
        "        \"--max-reads\", str(MAX_READS),\n",
        "        \"--emit-moves\",\n",
        "        \"--disable-read-splitting\"\n",
        "    ]\n",
        "    print(\"Running Dorado...\")\n",
        "    with open(bam_raw, \"wb\") as fh:\n",
        "        proc = subprocess.run(cmd, stdout=fh, stderr=subprocess.PIPE, text=True)\n",
        "    if proc.returncode != 0:\n",
        "        print(f\"Dorado error:\\n{proc.stderr[:500]}\")\n",
        "        continue\n",
        "    print(f\"Raw BAM created: {bam_raw}\")\n",
        "\n",
        "    print(\"Sorting BAM...\")\n",
        "    rc, out, err = run_cmd([\"samtools\", \"sort\", \"-o\", bam_sorted, bam_raw])\n",
        "    if rc != 0:\n",
        "        print(\"Failed to sort BAM\")\n",
        "        continue\n",
        "\n",
        "    print(\"Indexing BAM...\")\n",
        "    rc, out, err = run_cmd([\"samtools\", \"index\", bam_sorted])\n",
        "    if rc != 0:\n",
        "        print(\"Failed to index BAM\")\n",
        "\n",
        "    try:\n",
        "        os.remove(bam_raw)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    rc, out, _ = run_cmd([\"samtools\", \"view\", \"-c\", bam_sorted])\n",
        "    print(f\"Final sorted BAM: {bam_sorted} ({out.strip()} alignments)\")\n",
        "    bams.append(bam_sorted)\n",
        "\n",
        "merged_bam = os.path.join(OUTPUT_DIR, \"all_pod5s_merged.bam\")\n",
        "if len(bams) > 1:\n",
        "    rc, out, err = run_cmd([\"samtools\", \"merge\", \"-f\", merged_bam] + bams)\n",
        "    if rc == 0:\n",
        "        print(f\"Merged BAM created: {merged_bam}\")\n",
        "        rc, out, err = run_cmd([\"samtools\", \"index\", merged_bam])\n",
        "\n",
        "print(\"Processing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4pKVqDTmX1H",
        "outputId": "88216bbd-ea40-4072-f045-e01a2a6dc7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded raw signals for 340 reads (needed 340).\n",
            "Extraction done: seq=(1422974, 17, 7), sig=(1422974, 360), labels=(1422974,)\n",
            "Saved dataset: /content/deepsignal_paper/deepsignal_dataset_colab.h5\n",
            "PART B (Colab-optimized) complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import pysam\n",
        "from collections import defaultdict\n",
        "from pod5 import Reader\n",
        "import h5py\n",
        "\n",
        "# ===================== CONFIG (EDIT BEFORE RUN) =====================\n",
        "MERGED_BAM_PATH = \"/content/deepsignal_paper/all_pod5s_merged.bam\"  # output of PART A\n",
        "OUTPUT_DATASET = \"/content/deepsignal_paper/deepsignal_dataset_colab.h5\"\n",
        "WINDOW_SIZE = 17\n",
        "SIGNAL_LENGTH = 360\n",
        "MIN_COVERAGE = 5\n",
        "MAX_READS = 500       # <<-- safe test; increase or set None to run all reads (may OOM)\n",
        "MIN_MAPQ = 20           # mapping quality filter for reads\n",
        "ML_HIGH = 0.7           # Dorado prob threshold considered methylated\n",
        "ML_LOW = 0.3            # Dorado prob threshold considered unmethylated\n",
        "\n",
        "# ================ Utility helpers ================\n",
        "\n",
        "def safe_ml_probs(ml_tag):\n",
        "    \"\"\"Convert ML tag (bytes/list/int) to list of floats [0,1].\"\"\"\n",
        "    if ml_tag is None:\n",
        "        return None\n",
        "    # bytes: treat as uint8 array\n",
        "    if isinstance(ml_tag, (bytes, bytearray)):\n",
        "        arr = np.frombuffer(ml_tag, dtype=np.uint8)\n",
        "        return (arr.astype(np.float32) / 255.0).tolist()\n",
        "    # list/tuple of ints\n",
        "    try:\n",
        "        return (np.array(list(ml_tag), dtype=np.float32) / 255.0).tolist()\n",
        "    except Exception:\n",
        "        try:\n",
        "            # single int\n",
        "            return [float(ml_tag) / 255.0]\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "\n",
        "def parse_mm_tag_positions(mm_tag):\n",
        "    \"\"\"Robustly parse MM tag and return list of (base, positions[] as 0-based offsets in read).\n",
        "    We don't try to be perfectly spec-conforming for every odd variant; we aim to extract numeric positions.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    if not mm_tag:\n",
        "        return out\n",
        "    # mm_tag may be like: 'A+m,0,5;'\n",
        "    blocks = mm_tag.split(';')\n",
        "    for block in blocks:\n",
        "        if not block.strip():\n",
        "            continue\n",
        "        # split at first comma to get head like 'A+m' and remaining numeric tokens\n",
        "        parts = block.split(',')\n",
        "        head = parts[0]\n",
        "        # base info could be like 'A+m' or 'C+m' or '6mA' variants; keep head\n",
        "        base = head.split(':')[0] if ':' in head else head\n",
        "        # collect digits from the rest\n",
        "        pos_tokens = []\n",
        "        for tok in parts[1:]:\n",
        "            tok = tok.strip().lstrip('+')\n",
        "            m = re.match(r'-?(\\d+)', tok)\n",
        "            if m:\n",
        "                pos_tokens.append(int(m.group(1)))\n",
        "        # Dorado/Tombo spec: these are deltas (cumulative)\n",
        "        positions = []\n",
        "        offset = 0\n",
        "        for v in pos_tokens:\n",
        "            offset += v\n",
        "            positions.append(offset)\n",
        "        out.append((base, positions))\n",
        "    return out\n",
        "\n",
        "\n",
        "# ================ POD5 selective loader ================\n",
        "\n",
        "\n",
        "def load_pod5_signals_for_reads(pod5_glob, read_id_set):\n",
        "    \"\"\"Load only required reads' raw signals into dict {str(read_id): norm_signal}.\n",
        "    Normalizes per-read using MAD (paper).\n",
        "    \"\"\"\n",
        "    raw_signals = {}\n",
        "    pod5_list = sorted(glob.glob(pod5_glob))\n",
        "    if not pod5_list:\n",
        "        raise FileNotFoundError(f\"No POD5 files found at: {pod5_glob}\")\n",
        "\n",
        "    # normalize read_id_set to strings\n",
        "    wanted = set(str(x) for x in read_id_set)\n",
        "\n",
        "    for p5 in pod5_list:\n",
        "        with Reader(p5) as r:\n",
        "            for read in r.reads():\n",
        "                rid = str(read.read_id)  # <-- CAST TO STRING\n",
        "                if rid in wanted and rid not in raw_signals:\n",
        "                    sig = np.array(read.signal, dtype=np.float32)\n",
        "                    median = np.median(sig)\n",
        "                    mad = np.median(np.abs(sig - median))\n",
        "                    if mad > 0:\n",
        "                        norm = (sig - median) / mad\n",
        "                    else:\n",
        "                        norm = sig - median\n",
        "                    raw_signals[rid] = norm\n",
        "\n",
        "    return raw_signals\n",
        "\n",
        "\n",
        "\n",
        "# ================ MM/ML parsing to candidates (first pass) ================\n",
        "\n",
        "def parse_dorado_modifications_to_candidates(read, require_motif='GATC'):\n",
        "    \"\"\"\n",
        "    Return list of candidate dicts from a pysam read using MM/ML tags.\n",
        "    Candidate fields:\n",
        "      - read_id: str\n",
        "      - read_pos: 0-based position in read\n",
        "      - ref_pos: genome position\n",
        "      - probability: float (0..1)\n",
        "      - strand: '+' or '-'\n",
        "      - motif: sequence context around candidate\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "    if not read.has_tag('MM'):\n",
        "        return candidates\n",
        "\n",
        "    mm_tag = read.get_tag('MM')\n",
        "    ml_tag = read.get_tag('ML') if read.has_tag('ML') else None\n",
        "    ml_probs = safe_ml_probs(ml_tag)\n",
        "    seq = (read.query_sequence or \"\").upper()\n",
        "    ref_start = read.reference_start if read.reference_start is not None else 0\n",
        "\n",
        "    parsed = parse_mm_tag_positions(mm_tag)\n",
        "    ml_idx = 0\n",
        "\n",
        "    for base_head, positions in parsed:\n",
        "        # Only accept adenine-type mods (A or variants containing 'A' or '6mA')\n",
        "        if not re.search(r'A', base_head, re.I) and '6m' not in base_head.lower():\n",
        "            ml_idx += len(positions)\n",
        "            continue\n",
        "\n",
        "        for pos in positions:\n",
        "            if pos < 0 or pos >= len(seq):\n",
        "                ml_idx += 1\n",
        "                continue\n",
        "\n",
        "            # ================== FIX: Motif window ==================\n",
        "            ctx_len = 2  # number of bases before/after the candidate\n",
        "            ctx_start = max(0, pos - ctx_len)\n",
        "            ctx_end = min(len(seq), pos + ctx_len + 1)  # +1 because end index is exclusive\n",
        "            motif = seq[ctx_start:ctx_end]\n",
        "            # =======================================================\n",
        "\n",
        "            prob = ml_probs[ml_idx] if (ml_probs and ml_idx < len(ml_probs)) else 0.5\n",
        "            candidates.append({\n",
        "                'read_id': str(read.query_name),\n",
        "                'read_pos': pos,\n",
        "                'ref_pos': ref_start + pos,\n",
        "                'probability': float(prob),\n",
        "                'strand': '-' if read.is_reverse else '+',\n",
        "                'motif': motif\n",
        "            })\n",
        "\n",
        "            ml_idx += 1\n",
        "\n",
        "    return candidates\n",
        "\n",
        "\n",
        "# ================ Labeling & coverage helpers ================\n",
        "\n",
        "def check_your_coverage(bam_file, max_sites=200000):\n",
        "    coverage_per_site = defaultdict(int)\n",
        "    with pysam.AlignmentFile(bam_file, 'rb') as bam:\n",
        "        for read in bam.fetch():\n",
        "            if read.is_unmapped:\n",
        "                continue\n",
        "            # limit for speed\n",
        "            for pos in range(read.reference_start, read.reference_end):\n",
        "                coverage_per_site[pos] += 1\n",
        "            if len(coverage_per_site) > max_sites:\n",
        "                break\n",
        "    coverages = list(coverage_per_site.values())\n",
        "    if not coverages:\n",
        "        return []\n",
        "    return coverages\n",
        "\n",
        "\n",
        "def determine_labeling_strategy(bam_file):\n",
        "    coverages = check_your_coverage(bam_file)\n",
        "    if not coverages:\n",
        "        return 'dorado_only'\n",
        "    avg_coverage = np.mean(coverages)\n",
        "    high_cov_sites = sum(c >= MIN_COVERAGE for c in coverages)\n",
        "    total_sites = len(coverages)\n",
        "    if avg_coverage >= 10 and high_cov_sites > total_sites * 0.6:\n",
        "        return 'consensus_plus_dorado'\n",
        "    elif avg_coverage >= 5 and high_cov_sites > total_sites * 0.3:\n",
        "        return 'dorado_primary'\n",
        "    else:\n",
        "        return 'dorado_only'\n",
        "\n",
        "\n",
        "def simple_effective_labeling(candidates, strategy='dorado_only'):\n",
        "    site_probs = defaultdict(list)\n",
        "    for c in candidates:\n",
        "        site_probs[c['ref_pos']].append(c['probability'])\n",
        "    high_conf = []\n",
        "    # dorado thresholds\n",
        "    for c in candidates:\n",
        "        p = c['probability']\n",
        "        if p >= ML_HIGH:\n",
        "            c['label'] = 1; c['method'] = 'dorado_high'; high_conf.append(c)\n",
        "        elif p <= ML_LOW:\n",
        "            c['label'] = 0; c['method'] = 'dorado_low'; high_conf.append(c)\n",
        "        else:\n",
        "            c['label'] = None; c['method'] = 'ambiguous'\n",
        "    # consensus\n",
        "    if strategy in ('consensus_plus_dorado', 'dorado_primary'):\n",
        "        for c in candidates:\n",
        "            probs = site_probs[c['ref_pos']]\n",
        "            if len(probs) >= MIN_COVERAGE:\n",
        "                freq = np.mean(probs)\n",
        "                if freq == 1.0:\n",
        "                    c['label'] = 1; c['method'] = 'consensus_meth'\n",
        "                elif freq == 0.0:\n",
        "                    c['label'] = 0; c['method'] = 'consensus_unmeth'\n",
        "                if c not in high_conf:\n",
        "                    high_conf.append(c)\n",
        "    return high_conf\n",
        "\n",
        "# ================ Feature extraction (two-pass) ================\n",
        "\n",
        "def resample_to_length(arr, length):\n",
        "    \"\"\"Simple 1D resample (linear interpolation) to `length` values.\"\"\"\n",
        "    if len(arr) == 0:\n",
        "        return np.zeros(length, dtype=np.float32)\n",
        "    if len(arr) == length:\n",
        "        return arr.astype(np.float32)\n",
        "    x_old = np.linspace(0, 1, num=len(arr))\n",
        "    x_new = np.linspace(0, 1, num=length)\n",
        "    return np.interp(x_new, x_old, arr).astype(np.float32)\n",
        "\n",
        "\n",
        "def extract_paper_features_colab(bam_path=MERGED_BAM_PATH, pod5_glob=POD5_GLOB, max_reads=MAX_READS):\n",
        "    \"\"\"Memory-safe, paper-accurate feature extraction for Colab.\n",
        "    Returns: seq_features (n,17,7) where 7 = one-hot(4)+mean+std+count,\n",
        "             sig_features (n,360), labels (n,), metadata list\n",
        "    \"\"\"\n",
        "    # 2) first pass to collect candidate read_ids (and candidates) up to max_reads\n",
        "    bam = pysam.AlignmentFile(bam_path, 'rb')\n",
        "    candidate_list = []\n",
        "    read_ids_needed = set()\n",
        "    for i, read in enumerate(bam.fetch(until_eof=True)):\n",
        "        if max_reads and i >= max_reads:\n",
        "            break\n",
        "        if read.is_unmapped or (read.mapping_quality is not None and read.mapping_quality < MIN_MAPQ):\n",
        "            continue\n",
        "        cands = parse_dorado_modifications_to_candidates(read)\n",
        "        if cands:\n",
        "           candidate_list.extend(cands)\n",
        "           read_ids_needed.update(str(c['read_id']) for c in cands)\n",
        "\n",
        "\n",
        "    bam.close()\n",
        "\n",
        "    if not candidate_list:\n",
        "        return np.empty((0, WINDOW_SIZE, 7), dtype=np.float32), np.empty((0, SIGNAL_LENGTH), dtype=np.float32), np.empty(0, dtype=int), []\n",
        "\n",
        "    # 3) load only required pod5 signals\n",
        "    raw_signals = load_pod5_signals_for_reads(pod5_glob, read_ids_needed)\n",
        "    print(f\"Loaded raw signals for {len(raw_signals)} reads (needed {len(read_ids_needed)}).\")\n",
        "\n",
        "    # 4) second pass: compute feature matrices\n",
        "    bam = pysam.AlignmentFile(bam_path, 'rb')\n",
        "    seq_features = []\n",
        "    sig_features = []\n",
        "    labels = []\n",
        "    metadata = []\n",
        "\n",
        "    # Build index: read_id -> list of candidate dicts (to avoid re-parsing MM again)\n",
        "    per_read_candidates = defaultdict(list)\n",
        "    for c in candidate_list:\n",
        "        per_read_candidates[str(c['read_id'])].append(c)\n",
        "\n",
        "    for i, read in enumerate(bam.fetch(until_eof=True)):\n",
        "        if max_reads and i >= max_reads:\n",
        "            break\n",
        "        rid = read.query_name\n",
        "        if rid not in per_read_candidates:\n",
        "            continue\n",
        "        seq = (read.query_sequence or \"\").upper()\n",
        "        n_bases = len(seq)\n",
        "        # get raw signal\n",
        "        if rid in raw_signals:\n",
        "            raw_sig = raw_signals[rid]\n",
        "            if read.is_reverse:\n",
        "                raw_sig = raw_sig[::-1]\n",
        "        else:\n",
        "            raw_sig = None\n",
        "        # event tags (if present)\n",
        "        ev = None\n",
        "        if read.has_tag('EV') and read.has_tag('ES') and read.has_tag('ED'):\n",
        "            try:\n",
        "                ev = np.array(read.get_tag('EV'), dtype=np.float32)\n",
        "                es = np.array(read.get_tag('ES'), dtype=np.float32)\n",
        "                ed = np.array(read.get_tag('ED'), dtype=np.float32)\n",
        "                # if reverse, flip to query-order\n",
        "                if read.is_reverse:\n",
        "                    ev = ev[::-1]; es = es[::-1]; ed = ed[::-1]\n",
        "            except Exception:\n",
        "                ev = None\n",
        "\n",
        "        # for each candidate in this read make features\n",
        "        for cand in per_read_candidates[rid]:\n",
        "            read_pos = cand['read_pos']\n",
        "            # ========== SEQUENCE FEATURES (17 x 7) ==========\n",
        "            start_seq = max(0, read_pos - (WINDOW_SIZE//2))\n",
        "            end_seq = min(n_bases, read_pos + (WINDOW_SIZE//2) + 1)\n",
        "            window_seq = seq[start_seq:end_seq]\n",
        "            # pad both sides to CENTER candidate at index WINDOW_SIZE//2\n",
        "            left_pad = (WINDOW_SIZE//2) - (read_pos - start_seq)\n",
        "            right_pad = WINDOW_SIZE - len(window_seq) - max(0, left_pad)\n",
        "            if left_pad > 0:\n",
        "                window_seq = ('N'*left_pad) + window_seq\n",
        "            if right_pad > 0:\n",
        "                window_seq = window_seq + ('N'*right_pad)\n",
        "\n",
        "            # Use event stats if present else compute from raw per-base slices\n",
        "            base_stats = []  # [(mean,std,count) per base]\n",
        "            if ev is not None and len(ev) >= n_bases:\n",
        "                # map base->event index (simple scaling)\n",
        "                ratio = len(ev) / max(1, n_bases)\n",
        "                for b_idx in range(WINDOW_SIZE):\n",
        "                    base_idx = start_seq + b_idx - (left_pad if left_pad>0 else 0)\n",
        "                    ev_idx = int(round(base_idx * ratio)) if base_idx>=0 and base_idx<n_bases else None\n",
        "                    if ev_idx is not None and 0 <= ev_idx < len(ev):\n",
        "                        m = float(ev[ev_idx])\n",
        "                        s = float(es[ev_idx]) if 'es' in locals() else 0.0\n",
        "                        d = float(ed[ev_idx]) if 'ed' in locals() else 0.0\n",
        "                    else:\n",
        "                        m = s = d = 0.0\n",
        "                    base_stats.append((m, s, d))\n",
        "            elif raw_sig is not None and n_bases > 0:\n",
        "                samples_per_base = max(1, int(round(len(raw_sig) / n_bases)))\n",
        "                for b_idx in range(WINDOW_SIZE):\n",
        "                    base_idx = start_seq + b_idx - (left_pad if left_pad>0 else 0)\n",
        "                    if base_idx < 0 or base_idx >= n_bases:\n",
        "                        base_stats.append((0.0, 0.0, 0.0))\n",
        "                    else:\n",
        "                        sstart = int(base_idx * samples_per_base)\n",
        "                        send = min(len(raw_sig), sstart + samples_per_base)\n",
        "                        slice_ = raw_sig[sstart:send] if send> sstart else np.array([], dtype=np.float32)\n",
        "                        if slice_.size == 0:\n",
        "                            base_stats.append((0.0,0.0,0.0))\n",
        "                        else:\n",
        "                            base_stats.append((float(np.mean(slice_)), float(np.std(slice_)), int(slice_.size)))\n",
        "            else:\n",
        "                # fallback zeros\n",
        "                base_stats = [(0.0,0.0,0.0)] * WINDOW_SIZE\n",
        "\n",
        "            # build one-hot + stats\n",
        "            seq_matrix = np.zeros((WINDOW_SIZE, 7), dtype=np.float32)\n",
        "            for b in range(WINDOW_SIZE):\n",
        "                base = window_seq[b] if b < len(window_seq) else 'N'\n",
        "                one_hot = [0.0, 0.0, 0.0, 0.0]\n",
        "                if base == 'A': one_hot[0] = 1.0\n",
        "                elif base == 'C': one_hot[1] = 1.0\n",
        "                elif base == 'G': one_hot[2] = 1.0\n",
        "                elif base == 'T': one_hot[3] = 1.0\n",
        "                m,s,d = base_stats[b]\n",
        "                seq_matrix[b, :4] = one_hot\n",
        "                seq_matrix[b, 4] = m\n",
        "                seq_matrix[b, 5] = s\n",
        "                seq_matrix[b, 6] = d\n",
        "\n",
        "            # ========== SIGNAL FEATURES (360) ==========\n",
        "            # Prefer EV if present (resample to 360), else resample raw_sig around center sample\n",
        "            if ev is not None and len(ev) > 0:\n",
        "                # center event index\n",
        "                ratio = len(ev) / max(1, n_bases)\n",
        "                center_event = int(round(read_pos * ratio))\n",
        "                start_e = max(0, center_event - (SIGNAL_LENGTH//2))\n",
        "                end_e = min(len(ev), start_e + SIGNAL_LENGTH)\n",
        "                window_ev = ev[start_e:end_e]\n",
        "                if len(window_ev) < SIGNAL_LENGTH:\n",
        "                    window_ev = np.pad(window_ev, (0, SIGNAL_LENGTH - len(window_ev)), 'constant')\n",
        "                sig_vec = resample_to_length(window_ev, SIGNAL_LENGTH)\n",
        "            elif raw_sig is not None:\n",
        "                # approximate center in samples\n",
        "                samples_per_base = max(1, int(round(len(raw_sig) / max(1, n_bases))))\n",
        "                center_sample = int(read_pos * samples_per_base)\n",
        "                start_s = max(0, center_sample - (SIGNAL_LENGTH//2))\n",
        "                end_s = min(len(raw_sig), start_s + SIGNAL_LENGTH)\n",
        "                window_sig = raw_sig[start_s:end_s]\n",
        "                if len(window_sig) < SIGNAL_LENGTH:\n",
        "                    window_sig = np.pad(window_sig, (0, SIGNAL_LENGTH - len(window_sig)), 'constant')\n",
        "                sig_vec = resample_to_length(window_sig, SIGNAL_LENGTH)\n",
        "            else:\n",
        "                sig_vec = np.zeros((SIGNAL_LENGTH,), dtype=np.float32)\n",
        "\n",
        "            # MAD normalization per-signal vector (guarded)\n",
        "            med = np.median(sig_vec)\n",
        "            mad = np.median(np.abs(sig_vec - med))\n",
        "            if mad > 0:\n",
        "                sig_vec = (sig_vec - med) / mad\n",
        "            else:\n",
        "                sig_vec = sig_vec - med\n",
        "\n",
        "            # collect\n",
        "            seq_features.append(seq_matrix)\n",
        "            sig_features.append(sig_vec)\n",
        "            labels.append(1 if cand.get('probability', 0.5) >= ML_HIGH else 0 if cand.get('probability',0.5) <= ML_LOW else -1)\n",
        "            metadata.append(cand)\n",
        "\n",
        "    bam.close()\n",
        "\n",
        "    seq_features = np.array(seq_features, dtype=np.float32) if seq_features else np.empty((0, WINDOW_SIZE, 7), dtype=np.float32)\n",
        "    sig_features = np.array(sig_features, dtype=np.float32) if sig_features else np.empty((0, SIGNAL_LENGTH), dtype=np.float32)\n",
        "    labels = np.array(labels, dtype=np.int32) if labels else np.empty((0,), dtype=np.int32)\n",
        "\n",
        "    print(f\"Extraction done: seq={seq_features.shape}, sig={sig_features.shape}, labels={labels.shape}\")\n",
        "    return seq_features, sig_features, labels, metadata\n",
        "\n",
        "# ================ Save utility ================\n",
        "\n",
        "def save_dataset(seq_data, sig_data, y_true, metadata, output_path=OUTPUT_DATASET):\n",
        "    with h5py.File(output_path, 'w') as f:\n",
        "        f.create_dataset('sequence_features', data=seq_data, compression='gzip')\n",
        "        f.create_dataset('signal_features', data=sig_data, compression='gzip')\n",
        "        f.create_dataset('labels', data=y_true, compression='gzip')\n",
        "        mg = f.create_group('metadata')\n",
        "        for i, meta in enumerate(metadata):\n",
        "            g = mg.create_group(f'sample_{i}')\n",
        "            for k, v in meta.items():\n",
        "                if isinstance(v, (str, int, float)):\n",
        "                    g.attrs[k] = v\n",
        "                elif isinstance(v, np.ndarray):\n",
        "                    g.create_dataset(k, data=v)\n",
        "        f.attrs['paper_reference'] = 'DeepSignal (paper-accurate features, adapted)'\n",
        "        f.attrs['window_size'] = WINDOW_SIZE\n",
        "        f.attrs['signal_length'] = SIGNAL_LENGTH\n",
        "        f.attrs['n_samples'] = seq_data.shape[0]\n",
        "    print(f\"Saved dataset: {output_path}\")\n",
        "\n",
        "# ================ Runner ================\n",
        "\n",
        "def run_part_b_colab(bam_path=MERGED_BAM_PATH, pod5_glob=POD5_GLOB, max_reads=MAX_READS, save_path=OUTPUT_DATASET):\n",
        "    seq, sig, labels, meta = extract_paper_features_colab(bam_path, pod5_glob, max_reads=max_reads)\n",
        "    if seq.shape[0] == 0:\n",
        "        print('No extracted samples — check MM tags / POD5 availability / motif filters')\n",
        "        return\n",
        "    save_dataset(seq, sig, labels, meta, save_path)\n",
        "    print('PART B (Colab-optimized) complete.')\n",
        "\n",
        "\n",
        "# If called as script, run with defaults (safe test)\n",
        "if __name__ == '__main__':\n",
        "    run_part_b_colab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiJ598pVyykV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y5pmA9HAmnFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45dc516-3614-428d-9ff1-be5aeab335ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 0.6929, Val Loss: 0.6938\n",
            "Epoch [2/10], Train Loss: 0.6908, Val Loss: 0.6950\n",
            "Epoch [3/10], Train Loss: 0.6886, Val Loss: 0.7015\n",
            "Epoch [4/10], Train Loss: 0.6873, Val Loss: 0.7029\n",
            "Epoch [5/10], Train Loss: 0.6865, Val Loss: 0.7146\n",
            "Epoch [6/10], Train Loss: 0.6847, Val Loss: 0.7005\n",
            "Epoch [7/10], Train Loss: 0.6832, Val Loss: 0.7117\n",
            "Epoch [8/10], Train Loss: 0.6819, Val Loss: 0.6946\n",
            "Epoch [9/10], Train Loss: 0.6805, Val Loss: 0.7002\n",
            "Epoch [10/10], Train Loss: 0.6796, Val Loss: 0.6978\n",
            "Test Results:\n",
            "Accuracy:  0.507\n",
            "Precision: 0.494\n",
            "Recall:    0.247\n",
            "F1-Score:  0.329\n",
            "Confusion Matrix:\n",
            "[[5573 1787]\n",
            " [5328 1748]]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "\n",
        "DATA_PATH = \"/content/deepsignal_paper/deepsignal_dataset_colab.h5\"\n",
        "SAVE_PATH = \"/content/deepsignal_paper/deepsignal_balanced.h5\"\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 10\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "class DeepSignalDataset(Dataset):\n",
        "    def __init__(self, h5_path, read_ids=None):\n",
        "        self.h5_path = h5_path\n",
        "        with h5py.File(h5_path, 'r') as f:\n",
        "            self.sequence_features = f['sequence_features'][:]\n",
        "            self.signal_features = f['signal_features'][:]\n",
        "            self.labels = f['labels'][:]\n",
        "            n = self.labels.shape[0]\n",
        "            self.read_ids = [f[f'metadata/sample_{i}'].attrs['read_id'] for i in range(n)]\n",
        "        if read_ids is not None:\n",
        "            id_set = set(read_ids)\n",
        "            self.indices = [i for i, rid in enumerate(self.read_ids) if rid in id_set]\n",
        "        else:\n",
        "            self.indices = list(range(len(self.labels)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        actual_idx = self.indices[idx]\n",
        "        seq = torch.tensor(self.sequence_features[actual_idx], dtype=torch.float32)\n",
        "        sig = torch.tensor(self.signal_features[actual_idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[actual_idx], dtype=torch.float32)\n",
        "        return seq, sig, label\n",
        "\n",
        "def split_data_by_read_ids(h5_path, test_size=0.2, val_size=0.1):\n",
        "    with h5py.File(h5_path, 'r') as f:\n",
        "        n_samples = f['labels'].shape[0]\n",
        "        read_ids = []\n",
        "        labels_list = []\n",
        "        for i in range(n_samples):\n",
        "            sample_meta = f[f'metadata/sample_{i}']\n",
        "            read_ids.append(sample_meta.attrs['read_id'])\n",
        "            labels_list.append(f['labels'][i])\n",
        "    read_id_to_samples = {}\n",
        "    for i, read_id in enumerate(read_ids):\n",
        "        if read_id not in read_id_to_samples:\n",
        "            read_id_to_samples[read_id] = []\n",
        "        read_id_to_samples[read_id].append({\n",
        "            'sample_index': i,\n",
        "            'label': labels_list[i]\n",
        "        })\n",
        "    unique_read_ids = list(read_id_to_samples.keys())\n",
        "    train_reads, test_reads = train_test_split(\n",
        "        unique_read_ids,\n",
        "        test_size=test_size,\n",
        "        random_state=RANDOM_SEED,\n",
        "        shuffle=True\n",
        "    )\n",
        "    train_reads, val_reads = train_test_split(\n",
        "        train_reads,\n",
        "        test_size=val_size/(1-test_size),\n",
        "        random_state=RANDOM_SEED,\n",
        "        shuffle=True\n",
        "    )\n",
        "    train_samples = sum(len(read_id_to_samples[rid]) for rid in train_reads)\n",
        "    val_samples = sum(len(read_id_to_samples[rid]) for rid in val_reads)\n",
        "    test_samples = sum(len(read_id_to_samples[rid]) for rid in test_reads)\n",
        "    train_set = set(train_reads)\n",
        "    val_set = set(val_reads)\n",
        "    test_set = set(test_reads)\n",
        "    assert len(train_set & val_set) == 0\n",
        "    assert len(train_set & test_set) == 0\n",
        "    assert len(val_set & test_set) == 0\n",
        "    return train_reads, val_reads, test_reads\n",
        "\n",
        "def create_balanced_dataset(h5_path, output_path, samples_per_class=35000):\n",
        "    with h5py.File(h5_path, 'r') as f:\n",
        "        labels = f['labels'][:]\n",
        "        read_ids = [f[f'metadata/sample_{i}'].attrs['read_id'] for i in range(len(labels))]\n",
        "        valid_mask = (labels == 0) | (labels == 1)\n",
        "        valid_indices = np.where(valid_mask)[0]\n",
        "        methylated_idx = valid_indices[labels[valid_indices] == 1]\n",
        "        unmethylated_idx = valid_indices[labels[valid_indices] == 0]\n",
        "        meth_size = min(samples_per_class, len(methylated_idx))\n",
        "        unmeth_size = min(samples_per_class, len(unmethylated_idx))\n",
        "        methylated_sample = np.random.choice(methylated_idx, meth_size, replace=False)\n",
        "        unmethylated_sample = np.random.choice(unmethylated_idx, unmeth_size, replace=False)\n",
        "        balanced_indices = np.concatenate([methylated_sample, unmethylated_sample])\n",
        "        sorted_indices = np.sort(balanced_indices)\n",
        "        seq_data = f['sequence_features'][sorted_indices]\n",
        "        sig_data = f['signal_features'][sorted_indices]\n",
        "        labels_data = f['labels'][sorted_indices]\n",
        "        metadata_list = []\n",
        "        for idx in sorted_indices:\n",
        "            meta_group = f[f'metadata/sample_{idx}']\n",
        "            metadata_list.append({\n",
        "                'read_id': meta_group.attrs['read_id'],\n",
        "                'motif': meta_group.attrs.get('motif', ''),\n",
        "                'probability': meta_group.attrs.get('probability', 0.5),\n",
        "                'read_pos': meta_group.attrs.get('read_pos', 0),\n",
        "                'ref_pos': meta_group.attrs.get('ref_pos', 0),\n",
        "                'strand': meta_group.attrs.get('strand', '+')\n",
        "            })\n",
        "    with h5py.File(output_path, 'w') as f_out:\n",
        "        shuffle_idx = np.random.permutation(len(seq_data))\n",
        "        f_out.create_dataset('sequence_features', data=seq_data[shuffle_idx], compression='gzip')\n",
        "        f_out.create_dataset('signal_features', data=sig_data[shuffle_idx], compression='gzip')\n",
        "        f_out.create_dataset('labels', data=labels_data[shuffle_idx], compression='gzip')\n",
        "        meta_group = f_out.create_group('metadata')\n",
        "        for new_idx, old_idx in enumerate(shuffle_idx):\n",
        "            new_meta = meta_group.create_group(f'sample_{new_idx}')\n",
        "            for key, value in metadata_list[old_idx].items():\n",
        "                new_meta.attrs[key] = value\n",
        "    return output_path\n",
        "\n",
        "class DeepSignalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.signal_cnn_head = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, 7, stride=2, padding=3),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(3, stride=2, padding=1)\n",
        "        )\n",
        "        blocks = []\n",
        "        in_ch = 64\n",
        "        out_ch = 32\n",
        "        for _ in range(3):\n",
        "            blocks.append(InceptionBlock(in_ch, out_ch))\n",
        "            in_ch = out_ch * 4\n",
        "        self.signal_inception = nn.Sequential(*blocks)\n",
        "        self.signal_tail = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_ch, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.sequence_brnn = nn.Sequential(\n",
        "            nn.LSTM(7, 32, batch_first=True, bidirectional=True, num_layers=2),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, seq, sig):\n",
        "        sig = self.signal_cnn_head(sig.unsqueeze(1))\n",
        "        sig = self.signal_inception(sig)\n",
        "        sig = self.signal_tail(sig)\n",
        "        lstm_out, _ = self.sequence_brnn[0](seq)\n",
        "        seq_feat = self.sequence_brnn[1](lstm_out[:, -1, :])\n",
        "        combined = torch.cat([sig, seq_feat], dim=1)\n",
        "        return self.classifier(combined).squeeze()\n",
        "\n",
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Conv1d(in_channels, out_channels, 1)\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, 1),\n",
        "            nn.Conv1d(out_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, 1),\n",
        "            nn.Conv1d(out_channels, out_channels, 5, padding=2)\n",
        "        )\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool1d(3, stride=1, padding=1),\n",
        "            nn.Conv1d(in_channels, out_channels, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([\n",
        "            self.branch1(x),\n",
        "            self.branch2(x),\n",
        "            self.branch3(x),\n",
        "            self.branch4(x)\n",
        "        ], dim=1)\n",
        "\n",
        "def train_model():\n",
        "    balanced_path = create_balanced_dataset(DATA_PATH, SAVE_PATH, samples_per_class=35000)\n",
        "    train_reads, val_reads, test_reads = split_data_by_read_ids(balanced_path)\n",
        "    train_dataset = DeepSignalDataset(balanced_path, train_reads)\n",
        "    val_dataset = DeepSignalDataset(balanced_path, val_reads)\n",
        "    test_dataset = DeepSignalDataset(balanced_path, test_reads)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = DeepSignalModel().to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (seq, sig, labels) in enumerate(train_loader):\n",
        "            seq, sig, labels = seq.to(device), sig.to(device), labels.float().to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(seq, sig)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for seq, sig, labels in val_loader:\n",
        "                seq, sig, labels = seq.to(device), sig.to(device), labels.float().to(device)\n",
        "                outputs = model(seq, sig)\n",
        "                val_loss += criterion(outputs, labels).item()\n",
        "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for seq, sig, labels in test_loader:\n",
        "            seq, sig, labels = seq.to(device), sig.to(device), labels.float().to(device)\n",
        "            outputs = model(seq, sig)\n",
        "            preds = (outputs > 0.5).float()\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    print(f\"Test Results:\")\n",
        "    print(f\"Accuracy:  {accuracy:.3f}\")\n",
        "    print(f\"Precision: {precision:.3f}\")\n",
        "    print(f\"Recall:    {recall:.3f}\")\n",
        "    print(f\"F1-Score:  {f1:.3f}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    model = train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0GvxS5fmm93"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}